# RAG Backend: Technical & Mathematical Documentation

## 1. Overview
The **RAG (Retrieval-Augmented Generation)** backend serves as the intelligent core of the application, allowing users to query a structured database of Founders and Investors. It combines **Semantic Search** with **Generative AI** to provide answers that are both contextually accurate and linguistically natural.

---

## 2. Mathematical Foundation

The system relies on **Vector Space Models** to understand and match text.

### 2.1. Vector Embeddings
We utilize the `BAAI/bge-base-en-v1.5` model to map unstructured text (sentences, paragraphs) into a **768-dimensional vector space**.

Let $f_{\theta}$ be the embedding function parameterized by the model weights $\theta$. For any given input text $T$ (e.g., a query or a document), the vector representation $\mathbf{v}$ is given by:

$$
\mathbf{v} = f_{\theta}(T) \in \mathbb{R}^{d}
$$

Where $d = 768$.

**Normalization:**
To ensure stability and efficiency in distance calculations, all vectors are normalized to unit length on the hypersphere:

$$
\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{d} v_i^2} = 1
$$

### 2.2. Retrieval Metric: Cosine Similarity
To find the most relevant information for a user query $Q$, we calculate the semantic distance between the query vector $\mathbf{q}$ and every document vector $\mathbf{d}$ in our database.

We use **Cosine Similarity**, which measures the cosine of the angle $\alpha$ between two vectors:

$$
\text{Similarity}(\mathbf{q}, \mathbf{d}) = \cos(\alpha) = \frac{\mathbf{q} \cdot \mathbf{d}}{\|\mathbf{q}\| \|\mathbf{d}\|}
$$

### 2.3. Optimization: The Dot Product
Because we strictly enforce $L^2$ normalization (i.e., $\|\mathbf{q}\| = 1$ and $\|\mathbf{d}\| = 1$), the denominator becomes 1. This simplifies the computationally expensive Cosine Similarity into a pure **Dot Product**:

$$
\text{Score}(\mathbf{q}, \mathbf{d}) = \mathbf{q} \cdot \mathbf{d} = \sum_{i=1}^{768} q_i d_i
$$

This allows the ChromaDB engine (using HNSW formatting) to perform extremely fast approximate nearest neighbor searches.

---

## 3. The RAG Algorithm

The backend workflow follows a strict probabilistic pipeline:

### Step 1: Query Encoding
The user's natural language query $Q$ is transformed into a vector:
$$
\mathbf{q} \leftarrow \text{Encoder}(Q)
$$

### Step 2: Semantic Retrieval
We retrieve a set of top-$k$ documents $D_{retrieved}$ that maximize the similarity score.
$$
D_{retrieved} = \underset{d \in \text{Database}}{\text{arg max}_k} (\mathbf{q} \cdot \mathbf{d})
$$
*In our implementation, $k=3$ for founders and $k=3$ for investors.*

### Step 3: Context Aggregation
The retrieved documents are concatenated into a single context string $C$:
$$
C = d_1 \oplus d_2 \oplus \dots \oplus d_k
$$

### Step 4: Conditional Generation
The Final Response $R$ is generated by the LLM (Llama 3.2). The model predicts the probability of the next token $y_t$ conditioned not just on the query, but on the retrieved context $C$:

$$
P(R | Q, C) = \prod_{t=1}^{T} P(y_t | y_{<t}, Q, C)
$$

---

## 4. Architecture Implementation

### Tech Stack
- **Database**: ChromaDB (Vector Store)
- **Embedding Model**: BAAI/bge-base-en-v1.5
- **LLM**: Llama 3.2 (via Ollama)
- **Framework**: Python 3.12+

### Data Workflow
1. **Ingestion**: Raw CSV data (Founders/Investors) $\rightarrow$ Text Serialization $\rightarrow$ Vectorization $\rightarrow$ ChromaDB Storage.
2. **Inference**: User Query $\rightarrow$ Vectorization $\rightarrow$ Vector Search $\rightarrow$ Context Construction $\rightarrow$ LLM Generation.
